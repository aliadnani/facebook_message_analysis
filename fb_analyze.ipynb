{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37064bitcebcf6ff907a4c2e97528d87dcbc094c",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_json(path, name):\n",
    "    # Open Json\n",
    "    try:\n",
    "        with open(path) as json_data:\n",
    "            data = json.load(json_data)\n",
    "            # Fails here if json does not have messages object\n",
    "            # Filters out any other json files like settings.json \n",
    "            df = pd.DataFrame.from_dict(data['messages'])\n",
    "\n",
    "            df = df[df['sender_name'] == name]\n",
    "            return df\n",
    "    except:\n",
    "        print(\"Invalid JSON\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets path of any path that ends with json within script dir\n",
    "# Saves path as well as parent folder \n",
    "def get_paths():\n",
    "    path_obj = {}\n",
    "    cwd = str(os.getcwdb())[2:-1]\n",
    "    for root, dirs, files in os.walk(cwd):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                path_obj[os.path.join(root, file)] = os.path.join(root, file).split('\\\\')[-2][0:-11]\n",
    "    return(path_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Ali Adnan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mass dataframe of all messages jsons found using get_paths() \n",
    "def combine_dfs():\n",
    "    dfs = []\n",
    "    for key,value in get_paths().items():\n",
    "        # Populate dataframe with messages from {name} parsed from JSON\n",
    "        try:\n",
    "            data = parse_single_json(key, name)\n",
    "        # print(data)\n",
    "            data[\"sender\"] = value\n",
    "            dfs.append(data)\n",
    "        except:\n",
    "            pass\n",
    "    df_combined = pd.concat(dfs, sort=True)\n",
    "    return df_combined\n",
    "\n",
    "df_combined = combine_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more calculated collumns\n",
    "def add_calcfields(df_combined):\n",
    "\n",
    "    df_combined['timestamp_ms'] = pd.to_datetime(df_combined['timestamp_ms'], unit='ms') # set timestamp datatype\n",
    "    df_combined['date'] = df_combined['timestamp_ms'].apply(lambda x: (x + timedelta(hours=8)).date()) # calculate date from timestamp\n",
    "    df_combined['character_count'] = df_combined['content'].str.len() # calculate character count\n",
    "    df_combined['word_count'] = df_combined['content'].apply(lambda s : len(str(s).split(' '))) # calculate wordcount based on spaces\n",
    "    return df_combined\n",
    "\n",
    "df_combined = add_calcfields(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of week categories\n",
    "def add_dayofweek(df_combined):\n",
    "    df_combined['day_of_week'] = df_combined['timestamp_ms'].dt.day_name() # calculate day of week from timestamp\n",
    "    df_combined['day_of_week'] = pd.Categorical(df_combined['day_of_week'], categories=\n",
    "        ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday'],\n",
    "        ordered=True)\n",
    "    return df_combined\n",
    "\n",
    "df_combined = add_dayofweek(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punc(df_combined):\n",
    "    df_combined['content'] = df_combined['content'].apply(lambda x : str(x).replace(r'[^\\w\\s]+', ''))\n",
    "    return df_combined\n",
    "\n",
    "# df_combined = remove_punc(df_combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_count(df_combined,media_type):\n",
    "    df_combined = df_combined[df_combined[media_type].isnull() == 0]\n",
    "    df_combined = df_combined[media_type].apply(lambda x : len(x))\n",
    "    return df_combined\n",
    "\n",
    "df_photos = get_media_count(df_combined,'photos')\n",
    "df_videos = get_media_count(df_combined,'videos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all unneeded columns & reset index\n",
    "def drop_unneeded_cols(df_combined):\n",
    "    df_combined = df_combined.reset_index(drop=False)\n",
    "    print(df_combined)\n",
    "    df_combined = df_combined.drop(['audio_files', 'call_duration', 'files','gifs','missed','reactions','share','sticker','users','videos','photos','type','index'], axis=1)\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "df_combined = drop_unneeded_cols(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all content null rows\n",
    "def drop_null(df_combined):\n",
    "    df_combined = df_combined[df_combined[\"content\"].isnull() == 0]\n",
    "    return df_combined\n",
    "\n",
    "df_combined = drop_null(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_valuecounts(df_combined):\n",
    "    df_wordcount_rawlist = df_combined['content'].values.tolist()\n",
    "    df_wordcount_bigstring = \" \".join(df_wordcount_rawlist)\n",
    "    df_wordcount_list = df_wordcount_bigstring.split(\" \")\n",
    "    df_wordcount_rawseries = pd.Series(df_wordcount_list)\n",
    "    df_wordcount_rawseries = df_wordcount_rawseries.str.replace(r'[^\\w\\s]+', '')\n",
    "    df_wordcount_rawseries = df_wordcount_rawseries.str.lower()\n",
    "    df_wordcount_series = df_wordcount_rawseries.value_counts()\n",
    "    return df_wordcount_series\n",
    "# df_wordcount_list = ''\n",
    "df_wordcount_series = generate_word_valuecounts(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_wordcount_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top20_words(df_wordcount_series):\n",
    "    top20words = df_wordcount_series.iloc[0:20]\n",
    "    return top20words\n",
    "    \n",
    "top20words =  get_top20_words(df_wordcount_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_local_time(df_combined, locale, timedelta):\n",
    "    from datetime import datetime, timedelta\n",
    "    df_combined[f'timestamp_{locale}'] = df_combined['timestamp_ms'].apply(lambda x: (x + timedelta(hours=8))) # set timestamp datatype\n",
    "    return df_combined\n",
    "\n",
    "df_combined = add_local_time(df_combined,'hkt',(+8))\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make Graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "import plotly.graph_objects as go\n",
    "import psutil\n",
    "\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['hour_of_day'] = df_combined['timestamp_hkt'].apply(lambda x : x.strftime('%H'))\n",
    "df_hour_fig = df_combined.groupby(['hour_of_day'])['content'].count().iplot(dimensions=(900,600),colors=[\"DarkOrchid\",],kind='bar',title=\"Texts on Hour\",yTitle=\"Frequency\",xTitle=\"Hour\",asFigure=True)\n",
    "df_hour_fig.write_image(\"images/hour_msgs.svg\")\n",
    "df_hour_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_fig = df_combined.groupby(['date'])['content'].count().iplot(dimensions=(900,600),colors=[\"MediumTurquoise\",],kind='bar',title=\"Texts on Day\",yTitle=\"Frequency\",xTitle=\"Day\",asFigure=True)\n",
    "df_date_fig.write_image(\"images/date_msgs.svg\")\n",
    "df_date_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_rollingsum = df_combined.groupby(['date'])['content'].count().cumsum()\n",
    "df_date_rollingsum_fig = df_date_rollingsum.iplot(dimensions=(900,600),colors=[\"MediumSpringGreen\",],kind='area',fill=True,title=\"Cumulative Messages Over Time \",yTitle=\"Frequency\",xTitle=\"Day\",asFigure=True)\n",
    "df_date_rollingsum_fig.write_image(\"images/cumu_msgs.svg\")\n",
    "df_date_rollingsum_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_on_day = df_combined.groupby(['day_of_week'])['content'].count().iplot(dimensions=(900,600),colors=[\"Aquamarine\",],kind='bar',title=\"Messages on Day\",yTitle=\"Frequency\",xTitle=\"Day\",asFigure=True)\n",
    "messages_on_day.write_image(\"images/messages_on_day.svg\")\n",
    "messages_on_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20words_fig = top20words[0:-1].iplot(dimensions=(900,600),subplots=True,colors=[\"red\",],kind='bar',title=\"Most Commonly Used Words\",yTitle=\"Frequency\",xTitle=\"Word\",asFigure=True)\n",
    "top20words_fig.write_image(\"images/common_words.svg\")\n",
    "top20words_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make images folder\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wish there was a better way to make plot.ly text tables\n",
    "\n",
    "header = dict(values=['Metric', 'Value'])\n",
    "\n",
    "metric_col = [\"Total Number of Messages Sent\",\"Number of Photos Sent\",\"Number of Videos Sent\",\"Total Number of Words Sent\",\"Total Number of Characters Sent\",\"Average Number of Messages Sent per Day\",\"Average Word Count per Message\", \"Average Character Count per Message\"]\n",
    "\n",
    "values_col = [df_combined.content.count(),df_photos.sum(),df_videos.sum(),df_combined.character_count.sum(),df_combined.word_count.sum(),df_combined.groupby(['date'])['content'].count().mean(),df_combined.word_count.mean(), df_combined.character_count.mean()]\n",
    "\n",
    "data = [go.Table(header=header,cells=dict(align='left',values=[metric_col, values_col]))] \n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=600,\n",
    "    height=600,)\n",
    "fig.write_image(\"images/stats.svg\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "person_fig = df_combined.groupby(['sender'])['content'].count().reset_index(name='count').sort_values(['count'], ascending=False).set_index('sender').head(6)\n",
    "person_fig['sender_hidden'] = range(1, len(person_fig) + 1)\n",
    "person_fig['sender_hidden'] = person_fig['sender_hidden'].apply(lambda x : 'Person' + str(x))\n",
    "person_fig = person_fig.set_index('sender_hidden')\n",
    "print(person_fig)\n",
    "person_fig = person_fig.iplot(dimensions=(900,600),colors=[\"PaleGreen\",],kind='bar',title=\"Messages Sent to Person\",yTitle=\"Frequency\",xTitle=\"Person\",asFigure='True')\n",
    "person_fig.write_image(\"images/person.svg\")\n",
    "\n",
    "person_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markov_corpus(df_combined):\n",
    "    words_list = df_combined['content'].values.tolist()\n",
    "    corpus = '\\n'.join(words_list)\n",
    "    corpus = re.sub(r'[^\\w\\s]','',corpus)\n",
    "    corpus = corpus.lower()\n",
    "    return corpus\n",
    "\n",
    "corpus = generate_markov_corpus(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "text_model = markovify.NewlineText(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_array = []\n",
    "for i in range(10):\n",
    "    markov_array.append(text_model.make_short_sentence(320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(markov_array).to_excel('mm.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header=dict(align='left',values=['No.', 'Generated Markov Chain'])\n",
    "fig2 = go.Figure(data=[go.Table(header=header,cells=dict( align='left',values=[list(range(1,11)), markov_array]))])\n",
    "fig2.update_layout(width=800,height=800,)\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['month_year'] = df_combined['timestamp_hkt'].apply(lambda x : '{year}-{month}'.format(year=x.year, month=x.month))\n",
    "\n",
    "message_conciseness = df_combined.groupby(['month_year'])['word_count'].mean().iplot(dimensions=(900,600),colors=[\"Aquamarine\",],kind='bar',title=\"Message Words/Message over Time\",yTitle=\"Average Words/Message\",xTitle=\"Binned by Month\",asFigure=True)\n",
    "message_conciseness.write_image(\"images/message_conciseness.svg\")\n",
    "message_conciseness.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dateM_fig = df_combined.groupby(['month_year'])['content'].count().iplot(dimensions=(900,600),colors=[\"SeaGreen\",],kind='bar',title=\"Texts Binned by Month\",yTitle=\"Frequency\",xTitle=\"Date\",asFigure=True)\n",
    "df_dateM_fig.write_image(\"images/dateM_msgs.svg\")\n",
    "df_dateM_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM TEXT GENERATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import LSTM\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_corpus = df_combined['content'].values.tolist()\n",
    "# nlp_corpus = \".\".join(nlp_corpus).lower()\n",
    "# raw_text = nlp_corpus\n",
    "# import io\n",
    "# with io.open('aa.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = sorted(list(set(raw_text)))\n",
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# # summarize the loaded data\n",
    "# n_chars = len(raw_text)\n",
    "# n_vocab = len(chars)\n",
    "# print (\"Total Characters: \", n_chars)\n",
    "# print (\"Total Vocab: \", n_vocab)\n",
    "# # prepare the dataset of input to output pairs encoded as integers\n",
    "# seq_length = 100\n",
    "# dataX = []\n",
    "# dataY = []\n",
    "# for i in range(0, n_chars - seq_length, 1):\n",
    "# \tseq_in = raw_text[i:i + seq_length]\n",
    "# \tseq_out = raw_text[i + seq_length]\n",
    "# \tdataX.append([char_to_int[char] for char in seq_in])\n",
    "# \tdataY.append(char_to_int[seq_out])\n",
    "# n_patterns = len(dataX)\n",
    "# print (\"Total Patterns: \", n_patterns)\n",
    "# # reshape X to be [samples, time steps, features]\n",
    "# X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# # normalize\n",
    "# X = X / float(n_vocab)\n",
    "# # one hot encode the output variable\n",
    "# y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # define the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# # define the checkpoint\n",
    "# filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "# # fit the model\n",
    "# model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = np_utils.to_categorical(dataY)\n",
    "# # define the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# # load the network weights\n",
    "# filename = \"weights-improvement-01-2.7836.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# # pick a random seed\n",
    "# start = numpy.random.randint(0, len(dataX)-1)\n",
    "# pattern = dataX[start]\n",
    "# print (\"Seed:\")\n",
    "# print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# # generate characters\n",
    "# for i in range(1000):\n",
    "# \tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "# \tx = x / float(n_vocab)\n",
    "# \tprediction = model.predict(x, verbose=0)\n",
    "# \tindex = numpy.argmax(prediction)\n",
    "# \tresult = int_to_char[index]\n",
    "# \tseq_in = [int_to_char[value] for value in pattern]\n",
    "# \tsys.stdout.write(result)\n",
    "# \tpattern.append(index)\n",
    "# \tpattern = pattern[1:len(pattern)]\n",
    "# print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}