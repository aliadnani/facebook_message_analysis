{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37064bitcebcf6ff907a4c2e97528d87dcbc094c",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Roboto']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_json(path, name):\n",
    "    # Open Json\n",
    "    try:\n",
    "        with open(path) as json_data:\n",
    "            data = json.load(json_data)\n",
    "            df = pd.DataFrame.from_dict(data['messages'])\n",
    "            df = df[df['sender_name'] == name]\n",
    "            return df\n",
    "    except:\n",
    "        pass\n",
    "        print(\"JSON Loaded\")\n",
    "    # Create Dataframe from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths():\n",
    "    path_arr = {}\n",
    "    cwd = str(os.getcwdb())[2:-1]\n",
    "    for root, dirs, files in os.walk(cwd):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                #  path_arr.append(os.path.join(root, file))\n",
    "                path_arr[os.path.join(root, file)] = os.path.join(root, file).split('\\\\')[-2][0:-11]\n",
    "    return(path_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Ali Adnan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store dataframes of msg data\n",
    "dfs = []\n",
    "for key,value in get_paths().items():\n",
    "    # Populate dataframe with messages from {name} parsed from JSON\n",
    "    # print(key)\n",
    "    # print(value)\n",
    "    try:\n",
    "        data = parse_single_json(key, name)\n",
    "    # print(data)\n",
    "        data[\"sender\"] = value\n",
    "        dfs.append(data)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from dataframes dataframe into one dataframe\n",
    "df_combined = pd.concat(dfs, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more calculated collumns\n",
    "df_combined['timestamp_ms'] = pd.to_datetime(df_combined['timestamp_ms'], unit='ms') # set timestamp datatype\n",
    "df_combined['date'] = df_combined['timestamp_ms'].apply(lambda x: (x + timedelta(hours=8)).date()) # calculate date from timestamp\n",
    "df_combined['day_of_week'] = df_combined['timestamp_ms'].dt.day_name() # calculate day of week from timestamp\n",
    "df_combined['character_count'] = df_combined['content'].str.len() # calculate character count\n",
    "df_combined['word_count'] = df_combined['content'].apply(lambda s : len(str(s).split(' '))) # calculate wordcount based on spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by day\n",
    "df_combined['day_of_week'] = pd.Categorical(df_combined['day_of_week'], categories=\n",
    "    ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday'],\n",
    "    ordered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['content'] = df_combined['content'].apply(lambda x : str(x).replace(r'[^\\w\\s]+', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all unneeded columns & reset index\n",
    "df_combined = df_combined.reset_index(drop=False)\n",
    "print(df_combined)\n",
    "df_photos = df_combined[df_combined['photos'].isnull() == 0]\n",
    "df_videos = df_combined[df_combined['videos'].isnull() == 0]\n",
    "df_combined = df_combined.drop(['audio_files', 'call_duration', 'files','gifs','missed','reactions','share','sticker','users','videos','photos','type','index'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all content null rows\n",
    "df_combined = df_combined[df_combined[\"content\"].isnull() == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos = df_photos['photos'].apply(lambda x : len(x))\n",
    "print(df_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = df_videos['videos'].apply(lambda x : len(x))\n",
    "print(df_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combined.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcount_series = df_combined['content'].str.split(expand=True)\n",
    "print(df_wordcount_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcount_series = df_wordcount_series.stack()\n",
    "print(df_wordcount_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcount_series = df_wordcount_series.str.replace(r'[^\\w\\s]+', '')\n",
    "df_wordcount_series = df_wordcount_series.str.lower()\n",
    "print(df_wordcount_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcount_series = df_wordcount_series.value_counts()\n",
    "print(df_wordcount_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wordcount_series['fuck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20words = df_wordcount_series.iloc[0:20]\n",
    "print(top20words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxword = 'i'\n",
    "maxword2 = 'i'\n",
    "maxword3 = 'i'\n",
    "for i in range(len(df_wordcount_series)):\n",
    "    if len(df_wordcount_series.index[i]) > len(maxword):\n",
    "        maxword3 = maxword2\n",
    "        maxword2 = maxword\n",
    "\n",
    "        maxword = df_wordcount_series.index[i]\n",
    "\n",
    "print(maxword)\n",
    "print(maxword2)\n",
    "print(maxword3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxword = 'i'\n",
    "maxword2 = 'i'\n",
    "maxword3 = 'i'\n",
    "for i in range(len(df_wordcount_series)):\n",
    "    if len(df_wordcount_series.index[i]) > len(maxword):\n",
    "        maxword3 = maxword2\n",
    "        maxword2 = maxword\n",
    "\n",
    "        maxword = df_wordcount_series.index[i]\n",
    "\n",
    "print(maxword)\n",
    "print(maxword2)\n",
    "print(maxword3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "df_combined['timestamp_hkt'] = df_combined['timestamp_ms'].apply(lambda x: (x + timedelta(hours=8))) # set timestamp datatype\n",
    "\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['hour_of_day'] = df_combined['timestamp_hkt'].apply(lambda x : x.strftime('%H'))\n",
    "df_combined.groupby(['hour_of_day'])['content'].count().iplot(dimensions=(1500,1000),colors=[\"DarkOrchid\",],kind='bar',title=\"Texts on Hour\",yTitle=\"Frequency\",xTitle=\"Hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.groupby(['date'])['content'].count().iplot(dimensions=(1500,1000),colors=[\"MediumTurquoise\",],kind='bar',title=\"Texts on Day\",yTitle=\"Frequency\",xTitle=\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.groupby(['day_of_week'])['content'].count().iplot(dimensions=(1500,1000),colors=[\"Aquamarine\",],kind='bar',title=\"Messages on Day\",yTitle=\"Frequency\",xTitle=\"Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20words.iplot(dimensions=(1500,1000),subplots=True,colors=[\"red\",],kind='bar',title=\"Most Commonly Used Words\",yTitle=\"Frequency\",xTitle=\"Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=[go.Table(header=dict(values=['Metric', 'Value']),cells=dict( align='left',values=[[\"Total Number of Messages Sent\",\"Number of Photos Sent\",\"Number of Videos Sent\",\"Total Number of Words Sent\",\"Total Number of Characters Sent\",\"Average Number of Messages Sent per Day\",\"Average Word Count per Message\", \"Average Character Count per Message\"], [df_combined.content.count(),df_photos.sum(),df_videos.sum(),df_combined.character_count.sum(),df_combined.word_count.sum(),df_combined.groupby(['date'])['content'].count().mean(),df_combined.word_count.mean(), df_combined.character_count.mean()]]))])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined.groupby(['sender'])['content'].count().reset_index(name='count') \\\n",
    "#                              .sort_values(['count'], ascending=False) \\\n",
    "#                              .head(5).iplot(colors=[\"LightSeaGreen\",],kind='bar',title=\"Texts on Hour\",yTitle=\"Frequency\",xTitle=\"Hour\")\n",
    "\n",
    "df_combined.groupby(['sender'])['content'].count().reset_index(name='count').sort_values(['count'], ascending=False).set_index('sender').head(6).iplot(dimensions=(1500,1000),colors=[\"PaleGreen\",],kind='bar',title=\"Messages Sent to Person\",yTitle=\"Frequency\",xTitle=\"Person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"aaabaaa?\"\n",
    "a = a.replace(r'[^\\w\\s]+', '')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = df_combined['content'].values.tolist()\n",
    "corpus = '\\n'.join(words_list)\n",
    "corpus = re.sub(r'[^\\w\\s]','',corpus)\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "text_model = markovify.NewlineText(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_array = []\n",
    "for i in range(15):\n",
    "    markov_array.append(text_model.make_short_sentence(320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = go.Figure(data=[go.Table(header=dict(align='left',values=['No.', 'Generated Markov Chain']),cells=dict( align='left',values=[list(range(15)), markov_array]))])\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM TEXT GENERATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_corpus = df_combined['content'].values.tolist()\n",
    "nlp_corpus = \".\".join(nlp_corpus).lower()\n",
    "raw_text = nlp_corpus\n",
    "import io\n",
    "with io.open('aa.txt', \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # define the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# # define the checkpoint\n",
    "# filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "# # fit the model\n",
    "# model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-01-2.7836.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}